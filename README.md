

---

# ğŸµ Spotify ETL with Apache Airflow

This project extracts Spotify data, transforms it using Pandas, and loads it into Snowflake using Airflow DAGs.

---

## ğŸš€ Technologies Used
- **Apache Airflow** â€” Workflow orchestration
- **Pandas** â€” Data cleaning and transformation
- **Snowflake** â€” Cloud data warehouse
- **Python** â€” Core programming language
- **Ubuntu WSL2** â€” Development environment

---

## âš™ï¸ How to Run

1. **Set up virtual environment**
   ```bash
   python3 -m venv airflow_venv
   source airflow_venv/bin/activate
   ```

2. **Install required packages**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure Airflow**
   ```bash
   export AIRFLOW_HOME=~/MSP/airflow
   airflow db init
   airflow webserver --port 8080
   airflow scheduler
   ```

4. **Trigger the DAG**
   - Access Airflow UI (`http://localhost:8080`)
   - Enable and trigger the `spotify_pipeline` DAG manually

---

## ğŸ“‚ Folder Structure

```bash
MSP/
â”œâ”€â”€ airflow/          # Airflow home directory
â”‚   â””â”€â”€ dags/         # DAG files (spotify_pipeline.py)
â”œâ”€â”€ scripts/          # Python scripts for Extract, Transform, Load
â”œâ”€â”€ db/               # Database related files
â”œâ”€â”€ cleaned_artist_streams.csv
â”œâ”€â”€ cleaned_playlists.csv
â”œâ”€â”€ cleaned_playlists_tracks_streams.csv
â”œâ”€â”€ Dockerfile (optional)
â”œâ”€â”€ docker-compose.yml (optional)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Œ Notes

- **Virtual environment (`airflow_venv/`)** is not uploaded to GitHub.
- Snowflake credentials should be set via `.env` file.
- Tested in **Ubuntu WSL2** environment.



